#   使用二进制方式搭建K8S集群

+ centos初始化
+ 为etcd 和 apiserver 自签证书
+ 部署etcd集群
+ 部署master组件【安装docker、kube-apiserver、kube-controller-manager、kube-scheduler、etcd】
+ 部署node组件【安装kubelet、kube-proxy、docker、etcd】
+ 部署集群网络


### 准备虚拟机

| 主机名     | ip            |
| ---------- | ------------- |
| k8s_master | 192.168.1.170 |
| k8s_node   | 192.168.1.171 |



### 主机网络配置

~~~shell
[root@bogon ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens33
[root@bogon ~]# systemctl restart network
~~~

###  操作系统的初始化

#### 安装基础软件包

~~~shell
yum -y install wget bash-completion net-tools vim net-tools

yum install -y net-tools iproute lrzsz tree bridge-utils unzip bind-utils git gcc
~~~

~~~shell
#关闭防火墙
systemctl stop firewalld
systemctl disable firewalld

# 关闭selinux
# 永久关闭
sed -i 's/enforcing/disabled/' /etc/selinux/config  
# 临时关闭
setenforce 0  

# 关闭swap
# 临时
swapoff -a 
# 永久关闭
sed -ri 's/.*swap.*/#&/' /etc/fstab

# 根据规划设置主机名【master节点上操作】
hostnamectl set-hostname k8_master
# 根据规划设置主机名【node1节点操作】
hostnamectl set-hostname k8s_node1

# 在master添加hosts
cat >> /etc/hosts << EOF
192.168.1.170 k8s_master
192.168.1.171 k8s_node1
EOF

# 将桥接的IPv4流量传递到iptables的链
cat > /etc/sysctl.d/k8s.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
# 生效
sysctl --system 

# 时间同步
yum install ntpdate -y
ntpdate time.windows.com

# 同步时间
yum install -y chrony
systemctl restart chronyd && systemctl enable chronyd &&  systemctl status chronyd


~~~

###  部署Etcd集群

Etcd是一个分布式键值存储系统，Kubernetes使用Etcd进行数据存储，所以先准备一个Etcd数据库，为了解决Etcd单点故障，应采用集群方式部署，这里使用3台组建集群，可容忍一台机器故障，当然也可以使用5台组件集群，可以容忍2台机器故障

###  自签证书

K8S集群的内部，其实也是有证书的，如果不带证书，那么访问就会受限

###  准备cfssl证书生成工具

cfssl是一个开源的证书管理工具，使用json文件生成证书，相比openssl 更方便使用。找任意一台服务器操作，这里用Master节点。

~~~shell
## master执行
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64
chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64
mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
mv cfssl-certinfo_linux-amd64 /usr/bin/cfssl-certinfo

[root@k8_master ~]# mkdir /home/etcd
[root@k8_master ~]# cd !$
cd /home/etcd

[root@k8_master etcd]# cat > ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "www": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF


[root@k8_master etcd]# cat > ca-csr.json <<EOF
{
    "CN": "etcd CA",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing"
        }
    ]
}
EOF

[root@k8_master etcd]# cat > server-csr.json <<EOF
{
    "CN": "etcd",
    "hosts": [
    "192.168.1.170",
    "192.168.1.171"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
EOF

[root@k8_master etcd]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
[root@k8_master etcd]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=www server-csr.json | cfssljson -bare server

#上次etcd
[root@k8_master ~]# mkdir /opt/etcd/{bin,cfg,ssl} -p
[root@k8_master ~]# ll
总用量 16972
-rw-------. 1 root root     1257 12月 26 04:49 anaconda-ks.cfg
-rw-r--r--. 1 root root 17373071 1月   2 18:26 etcd-v3.4.12-linux-amd64.tar.gz

[root@k8_master ~]# tar -zxvf etcd-v3.4.12-linux-amd64.tar.gz  -C ./
[root@k8_master ~]# mv etcd-v3.4.12-linux-amd64/{etcd,etcdctl} /opt/etcd/bin/


[root@k8_master ~]# cat > /opt/etcd/cfg/etcd.conf <<EOF
#[Member]
ETCD_NAME="etcd1"
ETCD_DATA_DIR="/opt/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.1.170:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.1.170:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.1.170:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.1.170:2379"
ETCD_INITIAL_CLUSTER="etcd1=https://192.168.1.170:2380,etcd2=https://192.168.1.171:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
#ETCD_INITIAL_CLUSTER_STATE="new" 
EOF

[root@k8_master ~]# cat  > /usr/lib/systemd/system/etcd.service <<EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
WorkingDirectory=/opt/etcd/
EnvironmentFile=/opt/etcd/cfg/etcd.conf
ExecStart=/opt/etcd/bin/etcd \
--initial-cluster-state=new \
--cert-file=/opt/etcd/ssl/server.pem \
--key-file=/opt/etcd/ssl/server-key.pem \
--peer-cert-file=/opt/etcd/ssl/server.pem \
--peer-key-file=/opt/etcd/ssl/server-key.pem \
--trusted-ca-file=/opt/etcd/ssl/ca.pem \
--peer-trusted-ca-file=/opt/etcd/ssl/ca.pem
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

# 拷贝SSL证书
[root@k8_master etcd]# cd /home/etcd/
[root@k8_master etcd]# cp ca*pem server*pem /opt/etcd/ssl

# master执行
[root@k8_master ~]# scp /usr/lib/systemd/system/etcd.service root@192.168.1.171:/usr/lib/systemd/system/
[root@k8_master ~]# scp -r /opt/etcd/ root@192.168.1.171:/opt/


#k8s_node1执行
[root@k8s_node1 ~]# cat > /opt/etcd/cfg/etcd.conf<<EOF
> #[Member]
> ETCD_NAME="etcd2"
> ETCD_DATA_DIR="/opt/etcd/default.etcd"
> ETCD_LISTEN_PEER_URLS="https://192.168.1.171:2380"
> ETCD_LISTEN_CLIENT_URLS="https://192.168.1.171:2379"
>
> #[Clustering]
> ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.1.171:2380"
> ETCD_ADVERTISE_CLIENT_URLS="https://192.168.1.171:2379"
> ETCD_INITIAL_CLUSTER="etcd1=https://192.168.1.170:2380,etcd2=https://192.168.1.171:2380"
> ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
> EOF


#每个节点打开端口
firewall-cmd --zone=public --add-port=2380/tcp --permanent
firewall-cmd --zone=public --add-port=2379/tcp --permanent
firewall-cmd --reload

[root@k8_master etcd]# /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.1.170:2379,https://192.168.1.171:2379" endpoint health
[root@k8s_node1 ~]# /opt/etcd/bin/etcdctl --cacert=/opt/etcd/ssl/ca.pem --cert=/opt/etcd/ssl/server.pem --key=/opt/etcd/ssl/server-key.pem --endpoints="https://192.168.1.170:2379,https://192.168.1.171:2379" endpoint health


[root@k8_master bin]# /opt/etcd/bin/etcdctl \
--ca-file=/opt/etcd/ssl/ca.pem --cert-file=/opt/etcd/ssl/server.pem --key-file=/opt/etcd/ssl/server-key.pem \
--endpoints="https://192.168.1.170:2379,https://192.168.1.171:2379" \
cluster-health
~~~



### master节点部署

~~~shell
mkdir -p /k8s/ssl
cd /k8s/ssl
[root@k8_master ssl]# vi server-csr.json
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local",
      "192.168.1.170",
      "192.168.1.171"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}

[root@k8_master ssl]# vi ca-config.json
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}

[root@k8_master ssl]# vi ca-csr.json
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}

##生成ca证书
[root@k8_master ssl]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
[root@k8_master ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server

[root@k8_master ssl]# vi kube-proxy-csr.json
vi kube-proxy-csr.json
{
  "CN": "kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

[root@k8_master ssl]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

#上传kubernetes-server-linux-amd64.tar.gz
#解压
[root@k8_master ~]# tar zxvf kubernetes-server-linux-amd64.tar.gz
[root@k8_master ~]# mkdir -p /opt/kubernetes/bin/
[root@k8_master ~]# cd kubernetes/server/bin/
[root@k8_master bin]# cp kube-apiserver kube-scheduler kube-controller-manager kubectl /opt/kubernetes/bin/



[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserver
ExecStart=/opt/kubernetes/bin/kube-apiserver \$KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
~~~



### 在Node节点安装Docker

~~~shell

[root@k8s_node1 etcd]# yum install -y yum-utils device-mapper-persistent-data lvm2
[root@k8s_node1 etcd]# yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo
[root@k8s_node1 etcd]# yum install docker-ce -y
[root@k8s_node1 etcd]# curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://bc437cce.m.daocloud.io
[root@k8s_node1 etcd]# systemctl start docker
[root@k8s_node1 etcd]# systemctl enable docker

~~~



### 部署Flannel网络

~~~shell
##Falnnel要用etcd存储自身一个子网信息，所以要保证能成功连接Etcd，写入预定义子网段：
##在master节点执行以下命令
[root@k8_master bin]# cd /opt/etcd/ssl
[root@k8_master ssl]# /opt/etcd/bin/etcdctl \
--ca-file=ca.pem --cert-file=server.pem --key-file=server-key.pem \
--endpoints="https://192.168.1.170:2379,https://192.168.1.171:2379" \
set /coreos.com/network/config  '{ "Network": "172.17.0.0/16", "Backend": {"Type": "vxlan"}}'
~~~



~~~shell
##以下部署步骤在规划的每个node节点都操作。
##下载二进制包：
[root@k8s_node1 ~]# wget https://github.com/coreos/flannel/releases/download/v0.10.0/flannel-v0.10.0-linux-amd64.tar.gz
[root@k8s_node1 ~]# tar zxvf flannel-v0.13.0-linux-amd64.tar.gz
[root@k8s_node1 ~]# mkdir -pv /opt/kubernetes/bin
[root@k8s_node1 ~]# mv flanneld mk-docker-opts.sh /opt/kubernetes/bin

##配置Flannel：
[root@k8s_node1 ~]# mkdir -pv /opt/kubernetes/cfg/
[root@k8s_node1 ~]# vi /opt/kubernetes/cfg/flanneld
FLANNEL_OPTIONS="--etcd-endpoints=https://192.168.1.170:2379,https://192.168.1.171:2379 -etcd-cafile=/opt/etcd/ssl/ca.pem -etcd-certfile=/opt/etcd/ssl/server.pem -etcd-keyfile=/opt/etcd/ssl/server-key.pem"


##systemd管理Flannel：
[root@k8s_node1 ~]# vi /usr/lib/systemd/system/flanneld.service
[Unit]
Description=Flanneld overlay address etcd agent
After=network-online.target network.target
Before=docker.service

[Service]
Type=notify
EnvironmentFile=/opt/kubernetes/cfg/flanneld
ExecStart=/opt/kubernetes/bin/flanneld --ip-masq $FLANNEL_OPTIONS
ExecStartPost=/opt/kubernetes/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/subnet.env
Restart=on-failure

[Install]
WantedBy=multi-user.target

##以上保存

##配置Docker启动指定子网段：
[root@k8s_node1 ~]# vi /usr/lib/systemd/system/docker.service 
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
#修改
EnvironmentFile=/run/flannel/subnet.env
#修改
ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target

#保存

[root@k8s_node1 ~]# systemctl daemon-reload
[root@k8s_node1 ~]# systemctl start flanneld
[root@k8s_node1 ~]# systemctl enable flanneld
[root@k8s_node1 ~]# systemctl restart docker

[root@k8s_node1 ~]# ip a
4: flannel.1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue state UNKNOWN group default
    link/ether fa:e8:09:fa:18:6e brd ff:ff:ff:ff:ff:ff
    inet 172.17.15.0/32 brd 172.17.15.0 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::f8e8:9ff:fefa:186e/64 scope link
       valid_lft forever preferred_lft forever



~~~



### apiserver等组件的配置

~~~shell
##在部署Kubernetes之前一定要确保etcd、flannel、docker是正常工作的，否则先解决问题再继续
##master节点：
##创建CA证书：
[root@k8_master k8sv1.11]# vi ca-config.json
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}

[root@k8_master k8sv1.11]# vi ca-csr.json
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}

##生成apiserver证书：
[root@k8_master k8sv1.11]# vi  server-csr.json
{
    "CN": "kubernetes",
    "hosts": [
      "10.0.0.1",
      "127.0.0.1",
      "192.168.1.170",
      "192.168.1.171",
      "kubernetes",
      "kubernetes.default",
      "kubernetes.default.svc",
      "kubernetes.default.svc.cluster",
      "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "System"
        }
    ]
}

## 保存
##=============================================
"10.0.0.1",   //这个是后边dns要用的虚拟网络的网关,不用改,就用这个--service-cluster-ip-range=10.0.0.0/24 \ 切忌
      "127.0.0.1",
      "192.168.13.141",    #k8s集群的ip，也就是三台虚拟机的ip
      "192.168.13.142",
      "192.168.13.143",
##=============================================

##生成kube-proxy证书：
[root@k8_master k8sv1.11]# vi kube-proxy-csr.json
{
  "CN": "system:kube-proxy",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}

#保存

[root@k8_master k8sv1.11]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
[root@k8_master k8sv1.11]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server

[root@k8_master k8sv1.11]# cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

##最终生成以下证书文件：
[root@k8_master k8sv1.11]# ll *pem
-rw-------. 1 root root 1675 1月   3 21:14 ca-key.pem
-rw-r--r--. 1 root root 1359 1月   3 21:14 ca.pem
-rw-------. 1 root root 1679 1月   3 21:16 kube-proxy-key.pem
-rw-r--r--. 1 root root 1403 1月   3 21:16 kube-proxy.pem
-rw-------. 1 root root 1675 1月   3 21:15 server-key.pem
-rw-r--r--. 1 root root 1619 1月   3 21:15 server.pem

~~~



### 部署apiserver组件

~~~shell
##下载二进制包：https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md
##下载这个包（kubernetes-server-linux-amd64.tar.gz）就够了，包含了所需的所有组件。
[root@k8_master k8sv1.11]# tar zxvf kubernetes-server-linux-amd64.tar.gz
[root@k8_master k8sv1.11]# cd kubernetes/server/bin
[root@k8_master bin]# cp kube-apiserver kube-scheduler kube-controller-manager kubectl /opt/kubernetes/bin
[root@k8_master k8sv1.11]# cp server.pem  server-key.pem ca.pem ca-key.pem /opt/kubernetes/ssl/

#创建token文件，后面会用到：
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/token.csv
28ca89344ec4cebfe4a8e799c5ace5dc,kubelet-bootstrap,10001,"system:kubelet-bootstrap"
#第一列：随机字符串，自己可生成（最好不要改，后面会用到这串数字）
#第二列：用户名
#第三列：UID
#第四列：用户组
[root@k8_master k8sv1.11]# head -c 16 /dev/urandom | od -An -t x | tr -d ' '
28ca89344ec4cebfe4a8e799c5ace5dc

#创建apiserver配置文件
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/kube-apiserver
KUBE_APISERVER_OPTS="--logtostderr=true \
--v=4 \
--etcd-servers=https://192.168.1.170:2379,https://192.168.1.171:2379 \
--bind-address=192.168.1.170 \
--secure-port=6443 \
--advertise-address=192.168.1.170 \
--allow-privileged=true \
--service-cluster-ip-range=10.0.0.0/24 \
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \
--authorization-mode=RBAC,Node \
--enable-bootstrap-token-auth \
--token-auth-file=/opt/kubernetes/cfg/token.csv \
--service-node-port-range=30000-50000 \
--tls-cert-file=/opt/kubernetes/ssl/server.pem  \
--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \
--client-ca-file=/opt/kubernetes/ssl/ca.pem \
--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \
--etcd-cafile=/opt/etcd/ssl/ca.pem \
--etcd-certfile=/opt/etcd/ssl/server.pem \
--etcd-keyfile=/opt/etcd/ssl/server-key.pem"
#保存
============================================================
参数说明：
* --logtostderr 启用日志
* --v 日志等级
* --etcd-servers etcd集群地址
* --bind-address 监听地址（本机）
* --secure-port https安全端口
* --advertise-address 集群通告地址（本机）
* --allow-privileged 启用授权
* --service-cluster-ip-range Service虚拟IP地址段    //这里就用这个网段，切忌不要改
* --enable-admission-plugins 准入控制模块
* --authorization-mode 认证授权，启用RBAC授权和节点自管理
* --enable-bootstrap-token-auth 启用TLS bootstrap功能，后面会讲到
* --token-auth-file token文件
* --service-node-port-range Service Node类型默认分配端口范围
============================================================

#systemd管理apiserver
[root@k8_master k8sv1.11]# vi /usr/lib/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserver
ExecStart=/opt/kubernetes/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

#保存

[root@k8_master k8sv1.11]# systemctl daemon-reload
[root@k8_master k8sv1.11]# systemctl enable kube-apiserver
[root@k8_master k8sv1.11]# systemctl start kube-apiserver
[root@k8_master k8sv1.11]# systemctl status kube-apiserver.service
● kube-apiserver.service - Kubernetes API Server
   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)
   Active: active (running) since 日 2021-01-03 21:40:22 CST; 9s ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 16983 (kube-apiserver)
   CGroup: /system.slice/kube-apiserver.service
           └─16983 /opt/kubernetes/bin/kube-apiserver --logtostderr=true --v=4 --etcd-servers=https://192.168.1.170:2379,https://192.168.1.171:2379 --bind-address=192.168.1.170 --sec...


~~~



### 部署schduler组件

~~~shell
##部署schduler组件
##创建schduler配置文件
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/kube-scheduler
KUBE_SCHEDULER_OPTS="--logtostderr=true \
--v=4 \
--master=127.0.0.1:8080 \
--leader-elect"

#保存
##============================================
参数说明：
* --master 连接本地apiserver
* --leader-elect 当该组件启动多个时，自动选举（HA）
##============================================

##systemd管理schduler组件
[root@k8_master k8sv1.11]# vi cat /usr/lib/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/kube-scheduler
ExecStart=/opt/kubernetes/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

#保存

[root@k8_master k8sv1.11]# systemctl daemon-reload
[root@k8_master k8sv1.11]# systemctl enable kube-scheduler.service
[root@k8_master k8sv1.11]# systemctl start kube-scheduler.service
[root@k8_master k8sv1.11]# systemctl status kube-scheduler.service
● kube-scheduler.service - Kubernetes Scheduler
   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)
   Active: active (running) since 日 2021-01-03 16:27:44 CST; 6h ago
     Docs: https://github.com/kubernetes/kubernetes
 Main PID: 15376 (kube-scheduler)
   CGroup: /system.slice/kube-scheduler.service
           └─15376 /opt/kubernetes/bin/kube-scheduler --logtostderr=false --v=2 --log-dir=/opt/kubernetes/logs --leader-elect --master=127.0.0.1:8080 --address=127.0.0.1
        


~~~



### 部署controller-manager组件

~~~shell
##创建controller-manager配置文件
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/kube-controller-manager 
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=true \
--v=4 \
--master=127.0.0.1:8080 \
--leader-elect=true \
--address=127.0.0.1 \
--service-cluster-ip-range=10.0.0.0/24 \
--cluster-name=kubernetes \
--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \
--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \
--root-ca-file=/opt/kubernetes/ssl/ca.pem \
--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem"

##systemd管理controller-manager组件
[root@k8_master k8sv1.11]# vi /usr/lib/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-manager
ExecStart=/opt/kubernetes/bin/kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target

#保存

[root@k8_master k8sv1.11]# systemctl daemon-reload
[root@k8_master k8sv1.11]# systemctl enable kube-controller-manager
[root@k8_master k8sv1.11]# systemctl start kube-controller-manager
[root@k8_master k8sv1.11]# systemctl status kube-controller-manager.service
● kube-controller-manager.service - Kubernetes Controller Manager
   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)
   Active: active (running) since 日 2021-01-03 16:27:51 CST; 6h ago
     Docs: https://github.com/kubernetes/kubernetes


##所有组件都已经启动成功，通过kubectl工具查看当前集群组件状态
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok
etcd-0               Healthy   {"health": "true"}
etcd-1               Healthy   {"health": "true"}
controller-manager   Healthy   ok

~~~



### 在Node节点部署组件

~~~shell
##Master apiserver启用TLS认证后，Node节点kubelet组件想要加入集群，必须使用CA签发的有效证书才能与apiserver通信，当Node节点##很多时，签署证书是一件很繁琐的事情，因此有了TLS Bootstrapping机制，kubelet会以一个低权限用户自动向apiserver申请证书，##kubelet的证书由apiserver动态签署。


##master节点部署
##将kubelet-bootstrap用户绑定到系统集群角色
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl create clusterrolebinding kubelet-bootstrap \
  --clusterrole=system:node-bootstrapper \
  --user=kubelet-bootstrap
 
#创建kubeconfig文件:
#在生成kubernetes证书的目录下执行以下命令生成kubeconfig文件：
#指定apiserver 内网负载均衡地址
[root@k8_master k8sv1.11]# KUBE_APISERVER="https://10.206.176.19:6443"
[root@k8_master k8sv1.11]# BOOTSTRAP_TOKEN=28ca89344ec4cebfe4a8e799c5ace5dc

#设置客户端认证参数
[root@k8_master k8sv1.11]#/opt/kubernetes/bin/kubectl config set-credentials kubelet-bootstrap \
  --token=${BOOTSTRAP_TOKEN} \
  --kubeconfig=bootstrap.kubeconfig
  
#设置上下文参数
[root@k8_master k8sv1.11]#/opt/kubernetes/bin/kubectl config set-context default \
  --cluster=kubernetes \
  --user=kubelet-bootstrap \
  --kubeconfig=bootstrap.kubeconfig

##设置默认上下文
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

##创建kube-proxy kubeconfig文件
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl config set-cluster kubernetes \
  --certificate-authority=./ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl config set-credentials kube-proxy \
  --client-certificate=./kube-proxy.pem \
  --client-key=./kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig
  
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig
  
[root@k8_master k8sv1.11]# /opt/kubernetes/bin/kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

[root@k8_master k8sv1.11]# ll *kubeconfig
-rw-------. 1 root root  256 1月   3 23:33 bootstrap.kubeconfig
-rw-------. 1 root root 6273 1月   3 23:36 kube-proxy.kubeconfig

[root@k8_master k8sv1.11]# scp bootstrap.kubeconfig  kube-proxy.kubeconfig root@192.168.1.171:/opt/kubernetes/cfg/

##部署kubelet组件
##将前面下载的二进制包中的kubelet和kube-proxy拷贝到/opt/kubernetes/bin目录下。 
##（master节点）
[root@k8_master bin]# scp /root/k8sv1.11/kubernetes/server/bin kubelet kube-proxy 192.168.1.171:/opt/kubernetes/bin

##----------------------下面这些操作在所有node节点完成：---------------------------
##创建kubelet配置文件
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/kubelet
KUBELET_OPTS="--logtostderr=true \
--v=4 \
--hostname-override=192.168.1.170 \
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
--config=/opt/kubernetes/cfg/kubelet.config \
--cert-dir=/opt/kubernetes/ssl \
--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0"

##===============================================================
参数说明：
* --hostname-override 在集群中显示的主机名（你改成node1，node2也可以）
* --kubeconfig 指定kubeconfig文件位置，会自动生成
* --bootstrap-kubeconfig 指定刚才生成的bootstrap.kubeconfig文件
* --cert-dir 颁发证书存放位置
* --pod-infra-container-image 管理Pod网络的镜像
##===============================================================

[root@k8_master k8sv1.11]# scp /opt/kubernetes/cfg/kubelet 192.168.1.171:/opt/kubernetes/cfg/

##其中/opt/kubernetes/cfg/kubelet.config配置文件如下：
[root@k8_master k8sv1.11]# vi /opt/kubernetes/cfg/kubelet.config
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 192.168.1.170
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS: ["10.0.0.2"]
clusterDomain: cluster.local.
failSwapOn: false
authentication:
  anonymous:
    enabled: true
  webhook:
    enabled: false
    
 #保存
 
 [root@k8s_node1 bin]# vi /opt/kubernetes/cfg/kubelet
 KUBELET_OPTS="--logtostderr=true \
--v=4 \
--hostname-override=192.168.1.171 \
--kubeconfig=/opt/kubernetes/cfg/kubelet.kubeconfig \
--bootstrap-kubeconfig=/opt/kubernetes/cfg/bootstrap.kubeconfig \
--config=/opt/kubernetes/cfg/kubelet.config \
--cert-dir=/opt/kubernetes/ssl \
--pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google-containers/pause-amd64:3.0"

[root@k8_master k8sv1.11]# scp /opt/kubernetes/cfg/kubelet.config 192.168.1.171:/opt/kubernetes/cfg/


[root@k8s_node1 bin]# vi /opt/kubernetes/cfg/kubelet.config
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 192.168.1.171
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS: ["10.0.0.2"]
clusterDomain: cluster.local.
failSwapOn: false
authentication:
  anonymous:
    enabled: true
  webhook:
    enabled: false
 
##systemd管理kubelet组件
[root@k8s_node1 bin]# vi /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet
ExecStart=/opt/kubernetes/bin/kubelet $KUBELET_OPTS
Restart=on-failure
KillMode=process

[Install]
WantedBy=multi-user.target

[root@k8_master k8sv1.11]# vi /usr/lib/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
After=docker.service
Requires=docker.service

[Service]
EnvironmentFile=/opt/kubernetes/cfg/kubelet
ExecStart=/opt/kubernetes/bin/kubelet $KUBELET_OPTS
Restart=on-failure
KillMode=process

[Install]
WantedBy=multi-user.target

#保存


~~~

