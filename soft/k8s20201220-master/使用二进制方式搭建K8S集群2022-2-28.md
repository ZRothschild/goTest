## 操作系统初始化配置

#### 内核升级

```shell

sudo rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org

sudo rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm

sudo yum --disablerepo="*" --enablerepo="elrepo-kernel" list available

sudo yum --enablerepo=elrepo-kernel install kernel-ml

uname -sr

```

#### 设置 GRUB 默认的内核版本

为了让新安装的内核成为默认启动选项，你需要如下修改 GRUB 配置：

打开并编辑 /etc/default/grub并设置 GRUB_DEFAULT=0。意思是 GRUB 初始化页面的第一个内核将作为默认内核。

```shell

GRUB_TIMEOUT=5
GRUB_DEFAULT=0
GRUB_DISABLE_SUBMENU=true
GRUB_TERMINAL_OUTPUT="console"
GRUB_CMDLINE_LINUX="rd.lvm.lv=centos/root rd.lvm.lv=centos/swap crashkernel=auto rhgb quiet"
GRUB_DISABLE_RECOVERY="true"

# 执行 

sudo grub2-mkconfig -o /boot/grub2/grub.cfg

```

### 修改主机名

> ***注意*** 每一个节点设置相应的名称

```shell

sudo hostnamectl set-hostname master01
sudo hostnamectl set-hostname node01

```

### 添加hosts

```shell

sudo bash -c "cat >> /etc/hosts" << EOF
192.168.154.128   master01
192.168.154.129   node01
192.168.154.130   node02
EOF

```

### 所有机子执行，初始化动作

* wget 下载文件工具
* bash-completion 命令自动补全
* net-tools 网络工具箱
* vim 文本编辑器

```shell

sudo yum -y install wget bash-completion net-tools vim

```

### 关闭防火墙,内网可以关,公网不能关

```shell

sudo systemctl stop firewalld && sudo systemctl disable firewalld

```

### 关闭selinux

> 安全增强型 Linux（Security-Enhanced Linux）简称 SELinux，它是一个 Linux 内核模块，也是 Linux 的一个安全子系统。

```shell


setenforce 0
sed -i 's/^SELINUX=.\*/SELINUX=disabled/' /etc/selinux/config

## 或

sudo vim  /etc/selinux/config # 永久关闭

setenforce 0  # 临时关闭

# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=disable # 设置为第 disabled
# SELINUXTYPE= can take one of three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted


```

### 关闭linux交互分区

> 将/etc/fstab 文件中包含swap的行注释掉

```shell

sudo sed -i '/swap/s/^/#/' /etc/fstab
swapoff -a

# 永久关闭，修改/etc/fstab,注释掉swap一行

# /etc/fstab
# Created by anaconda on Mon Feb 14 17:50:05 2022
#
# Accessible filesystems, by reference, are maintained under '/dev/disk'
# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info
#
UUID=68800430-5485-4a7e-83c6-86970b207749 /                       xfs     defaults        0 0
UUID=ad4b16e6-e709-424b-be7d-1506c56f3372 /boot                   xfs     defaults        0 0
UUID=da73b19f-ed3f-42de-bd5a-3dc2df716e8b swap                    swap    defaults        0 0

```

### 时间同步

```shell

sudo yum install -y chrony
sudo systemctl start chronyd && sudo systemctl enable chronyd && sudo systemctl status chronyd
sudo chronyc sources

```

### 加载ipvs模块

```shell

sudo modprobe -- ip_vs && sudo modprobe -- ip_vs_rr && sudo modprobe -- ip_vs_wrr
sudo modprobe -- ip_vs_sh && sudo modprobe -- nf_conntrack_ipv4

# 或

vim /etc/rc.local 

modprobe -- ip_vs
modprobe -- ip_vs_sh
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- nf_conntrack_ipv4

lsmod | grep ip_vs
lsmod | grep nf_conntrack_ipv4
sudo yum install -y ipvsadm

```

### 修改内核参数 将桥接的IPv4流量传递到iptables的链

```shell

sudo bash -c "cat > /etc/sysctl.d/k8s.conf" << EOF
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

sudo sysctl --system

```

## master01执行

### 工作目录生成

```shell

# etcd 使用 k8s 使用
sudo mkdir -p ~/work/{etcd,k8s,tmp}
sudo mkdir -p /usr/local/etcd/{bin,cfg,ssl,logs}
sudo mkdir -p /usr/local/k8s/{bin,cfg,ssl,logs}

```

### 准备cfssl证书生成工具 可以去github 使用go安装

```shell

cd  ~/work/tmp

sudo wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssl_1.6.1_linux_amd64
sudo wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssljson_1.6.1_linux_amd64
sudo wget https://github.com/cloudflare/cfssl/releases/download/v1.6.1/cfssl-certinfo_1.6.1_linux_amd64

sudo chmod +x cfssl_1.6.1_linux_amd64 cfssljson_1.6.1_linux_amd64 cfssl-certinfo_1.6.1_linux_amd64

sudo mv cfssl_1.6.1_linux_amd64 /usr/local/bin/cfssl
sudo mv cfssljson_1.6.1_linux_amd64 /usr/local/bin/cfssljson
sudo mv cfssl-certinfo_1.6.1_linux_amd64 /usr/bin/cfssl-certinfo

```

### 证书配置字段意义

* 公用名称 (Common Name) 简称：CN 字段，对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端证书则为证书申请者的姓名
* 单位名称 (Organization Name) 简称：O 字段，对于 SSL 证书，一般为网站域名；而对于代码签名证书则为申请单位名称；而对于客户端单位证书则为证书申请者所在单位名称

#### 证书申请单位所在地

|  字段名   | 字段值  |
|  ----  | ----  |
| 所在城市 (Locality)  | 简称：L 字段 |
| 所在省份 (State/Provice) | 简称：S 字段 |
| 所在国家  (Country) | 简称：C 字段，只能是国家字母缩写，如中国：CN |

#### 证书其他字段

|  字段名   | 字段值  |
|  ----  | ----  |
| 电子邮件 (Email)  | 简称：E 字段 |
| 多个姓名字段 | 简称：G 字段 |
| 介绍 | Description 字段 | 
| 电话号码 | Phone 字段 格式要求 + 国家区号 城市区号 电话号码，如： +86 732 88888888 | 
| 地址  | STREET 字段 | 
| 邮政编码  | PostalCode 字段 | 
| 显示其他内容  | 简称:OU 字段 | 

### 搭建etcd集群

***注：***

* CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；
* O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)

#### etcd 配置ca请求文件

```shell

cd ~/work/etcd

# 配置ca请求文件
sudo bash -c "cat > ~/work/etcd/ca-csr.json" << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "system"
        }
    ]
}
EOF

# 创建ca证书
cfssl gencert -initca ~/work/etcd/ca-csr.json  | cfssljson -bare ca

ll *pem

# -rw-------. 1 root root 1675 1月   4 20:37 ca-key.pem
# -rw-r--r--. 1 root root 1265 1月   4 20:37 ca.pem

# 配置ca证书策略
sudo bash -c "cat > ~/work/etcd/ca-config.json" << EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "kubernetes": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF

# 配置etcd请求csr文件

sudo bash -c "cat > ~/work/etcd/etcd-csr.json" << EOF
{
    "CN": "kubernetes",
    "hosts": [
    "192.168.154.125",
    "192.168.154.126",
    "192.168.154.127",
    "192.168.154.128",
    "192.168.154.129",
    "192.168.154.130",
    "192.168.154.131",
    "192.168.154.132",
    "192.168.154.133",
    "192.168.154.134",
    "192.168.154.135",
    "192.168.154.136",
    "192.168.154.137",
    "192.168.154.138",
    "192.168.154.139"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing"
        }
    ]
}
EOF

# etcd生成证书
sudo cfssl gencert -ca=~/work/etcd/ca.pem -ca-key=~/work/etcd/ca-key.pem -config=~/work/etcd/ca-config.json -profile=kubernetes ~/work/etcd/etcd-csr.json | cfssljson  -bare etcd

ll etcd*.pem

# -rw-------. 1 root root 1675 1月   4 20:37 etcd-key.pem
# -rw-r--r--. 1 root root 1265 1月   4 20:37 etcd.pem

```

#### 下部署etcd集群

#### 下载etcd软件包

```shell


# 复制刚刚生成的证书
sudo mv ~/work/etcd/ca*.pem ~/work/etcd/etcd*.pem  /usr/local/etcd/ssl

# 查看是否已经复制成功

ll /usr/local/etcd/ssl

cd ~/work/etcd

sudo wget https://github.com/etcd-io/etcd/releases/download/v3.4.13/etcd-v3.4.13-linux-amd64.tar.gz

sudo tar -zxvf etcd-v3.4.13-linux-amd64.tar.gz

# 可以查看是否变成执行权限
sudo cp -p ~/work/etcd/etcd-v3.4.13-linux-amd64/etcd* /usr/local/etcd/bin/

```

#### 创建配置文件

```shell

sudo bash -c "cat > /usr/local/etcd/cfg/etcd.conf" << EOF
#[Member]
ETCD_NAME="etcd-1"
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="https://192.168.154.128:2380"
ETCD_LISTEN_CLIENT_URLS="https://192.168.154.128:2379"

#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="https://192.168.154.128:2380"
ETCD_ADVERTISE_CLIENT_URLS="https://192.168.154.128:2379"
ETCD_INITIAL_CLUSTER="etcd-1=https://192.168.154.128:2380,etcd-2=https://192.168.154.129:2380,etcd-3=https://192.168.154.130:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"
ETCD_INITIAL_CLUSTER_STATE="new"
EOF

```

***注：***

|  字段名   | 字段值  |
|  ----  | ----  |
| ETCD_NAME  | 节点名称，集群中唯一 |
| ETCD_DATA_DIR | 数据目录 |
| ETCD_LISTEN_PEER_URLS | 集群通信监听地址 |
| ETCD_INITIAL_ADVERTISE_PEER_URLS | 集群通告地址 |
| ETCD_ADVERTISE_CLIENT_URLS  | 客户端通告地址 |
| ETCD_INITIAL_CLUSTER | 集群节点地址 |
| ETCD_INITIAL_CLUSTER_TOKEN  | 集群Token |
| ETCD_INITIAL_CLUSTER_STATE  | 加入集群的当前状态，new是新集群，existing表示加入已有集群 |

#### systemd管理etcd

***注意***

```shell

# etcd 4.3 会自动读取配置文件数据
--name=\${ETCD_NAME} \
--data-dir=\${ETCD_DATA_DIR} \
--listen-peer-urls=\${ETCD_LISTEN_PEER_URLS} \
--listen-client-urls=\${ETCD_LISTEN_CLIENT_URLS} \
--advertise-client-urls=\${ETCD_ADVERTISE_CLIENT_URLS} \
--initial-advertise-peer-urls=\${ETCD_INITIAL_ADVERTISE_PEER_URLS} \
--initial-cluster=\${ETCD_INITIAL_CLUSTER} \
--initial-cluster-token=\${ETCD_INITIAL_CLUSTER_TOKEN} \
--initial-cluster-state=\${ETCD_INITIAL_CLUSTER_STATE}  \

```

```shell

sudo bash -c  "cat > /usr/lib/systemd/system/etcd.service" << EOF
[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target

[Service]
Type=notify
EnvironmentFile=/usr/local/etcd/cfg/etcd.conf
ExecStart=/usr/local/etcd/bin/etcd \
--cert-file=/usr/local/etcd/ssl/etcd.pem \
--key-file=/usr/local/etcd/ssl/etcd-key.pem \
--peer-cert-file=/usr/local/etcd/ssl/etcd.pem \
--peer-key-file=/usr/local/etcd/ssl/etcd-key.pem \
--trusted-ca-file=/usr/local/etcd/ssl/ca.pem \
--peer-trusted-ca-file=/usr/local/etcd/ssl/ca.pem \
--logger=zap
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

```

### 同步相关文件到各个节点

***注意***

* 所有节点修改etcd.conf里面信息ip与etcd名称

```shell

# etcd 配置文件 证书
sudo scp -r /usr/local/etcd/  root@192.168.154.129:/usr/local/
sudo scp -r /usr/local/etcd/  root@192.168.154.130:/usr/local/

# 启动文件
sudo scp  /usr/lib/systemd/system/etcd.service  root@192.168.154.129:/usr/lib/systemd/system/
sudo scp  /usr/lib/systemd/system/etcd.service  root@192.168.154.130:/usr/lib/systemd/system/

# 所有节点启动执行
sudo systemctl daemon-reload && sudo systemctl enable etcd && sudo systemctl start etcd
sudo systemctl status etcd -l

# 查看集群状态
sudo ETCDCTL_API=3 /usr/local/etcd/bin/etcdctl --cacert=/usr/local/etcd/ssl/ca.pem --cert=/usr/local/etcd/ssl/etcd.pem --key=/usr/local/etcd/ssl/etcd-key.pem --endpoints="https://192.168.154.128:2379,https://192.168.154.129:2379,https://192.168.154.130:2379" endpoint health

# sudo ETCDCTL_API=3 /usr/local/etcd/bin/etcdctl --cacert=/usr/local/etcd/ssl/ca.pem --cert=/usr/local/etcd/ssl/etcd.pem --key=/usr/local/etcd/ssl/etcd-key.pem --endpoints="https://192.168.154.128:2379,https://192.168.154.129:2379,https://192.168.154.130:2379" endpoint status --write-out=table
# sudo ETCDCTL_API=3 /usr/local/etcd/bin/etcdctl --cacert=/usr/local/etcd/ssl/ca.pem --cert=/usr/local/etcd/ssl/etcd.pem --key=/usr/local/etcd/ssl/etcd-key.pem --endpoints="https://192.168.154.128:2379,https://192.168.154.129:2379,https://192.168.154.130:2379" get /test/testkey # put 插入 get 是获取

```

### 安装Docker在node服务器安装

* https://docs.docker.com/engine/install/centos/
* 使用非 root 用户 https://docs.docker.com/engine/install/linux-postinstall/

```shell

cd ~/work/tmp
sudo wget https://download.docker.com/linux/static/stable/x86_64/docker-20.10.9.tgz
sudo tar zxvf docker-20.10.9.tgz
sudo mv docker/* /usr/bin/

```

#### systemd管理docker

```shell

sudo bash -c  "cat > /usr/lib/systemd/system/docker.service" << EOF
[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target firewalld.service
Wants=network-online.target

[Service]
Type=notify
ExecStart=/usr/bin/dockerd
ExecReload=/bin/kill -s HUP $MAINPID
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity
TimeoutStartSec=0
Delegate=yes
KillMode=process
Restart=on-failure
StartLimitBurst=3
StartLimitInterval=60s

[Install]
WantedBy=multi-user.target
EOF

```

#### 下载依赖镜像node节点

```shell

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2 k8s.gcr.io/pause:3.2
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.2

docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0
docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0 k8s.gcr.io/coredns:1.7.0
docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.7.0

```

#### 创建配置文件

```shell

cat > /etc/docker/daemon.json << EOF 
{
  "exec-opts": [
    "native.cgroupdriver=systemd"
  ],
  "registry-mirrors": [
    "https://b9pmyelo.mirror.aliyuncs.com",
    "https://docker.mirrors.ustc.edu.cn",
    "https://1nj0zren.mirror.aliyuncs.com",
    "https://kfwkfulq.mirror.aliyuncs.com",
    "https://2lqq34jg.mirror.aliyuncs.com",
    "https://pee6w651.mirror.aliyuncs.com",
    "https://registry.docker-cn.com"
  ]
}
EOF

# 启动docker 
sudo sudo systemctl daemon-reload && sudo  systemctl enable docker && sudo systemctl start docker

```

## k8s部署

### k8s部署master节点

```shell

cd ~/work/tmp

sudo wget https://dl.k8s.io/v1.20.1/kubernetes-server-linux-amd64.tar.gz

sudo tar -zxvf kubernetes-server-linux-amd64.tar

cd kubernetes/server/bin/

sudo cp kube-apiserver kube-controller-manager kube-scheduler /usr/local/k8s/bin/
sudo cp kubectl /usr/local/bin/

cd ~/work/k8s

sudo bash -c "cat > ~/work/k8s/ca-csr.json" << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "Beijing",
            "ST": "Beijing",
            "O": "k8s",
            "OU": "system"
        }
    ]
}
EOF

sudo bash -c "cat > ~/work/k8s/ca-config.json" << EOF
{
    "signing": {
        "default": {
            "expiry": "87600h"
        },
        "profiles": {
            "kubernetes": {
                "expiry": "87600h",
                "usages": [
                    "signing",
                    "key encipherment",
                    "server auth",
                    "client auth"
                ]
            }
        }
    }
}
EOF

cfssl gencert -initca  ~/work/k8s/ca-csr.json | cfssljson -bare ca -

# 查看生成文件
ll ca.*pem

#-rw-------. 1 root root 1679 1月   4 21:45 ca-key.pem
#-rw-r--r--. 1 root root 1359 1月   4 21:45 ca.pem

```

### 创建csr请求文件

***注意***

* 如果 hosts 字段不为空则需要指定授权使用该证书的IP或域名列表。 由于该证书后续被 kubernetes master 集群使用，需要将master节点的IP都填上，同时还需要填写service网络的首个IP。
* (一般是 kube-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.0.0.1)

```shell

sudo bash -c "cat > ~/work/k8s/kube-apiserver-csr.json" << EOF
{
    "CN": "kubernetes",
    "hosts": [
        "10.0.0.1",
        "127.0.0.1",
        "192.168.1.170",
        "192.168.1.171",
        "192.168.1.172",
        "192.168.1.173",
        "192.168.1.174",
        "192.168.1.175",
        "192.168.1.176",
        "192.168.56.133",
        "192.168.56.134",
        "192.168.56.135",
        "192.168.56.136",
        "192.168.56.137",
        "192.168.56.138",
        "192.168.56.139",
        "192.168.56.140",
        "192.168.56.141",
        "192.168.56.142",
        "192.168.56.143",
        "192.168.56.145",
        "192.168.154.125",
        "192.168.154.126",
        "192.168.154.127",
        "192.168.154.128",
        "192.168.154.129",
        "192.168.154.130",
        "192.168.154.131",
        "192.168.154.132",
        "192.168.154.133",
        "192.168.154.134",
        "192.168.154.135",
        "192.168.154.136",
        "192.168.154.137",
        "192.168.154.138",
        "192.168.154.139",
        "kubernetes",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "k8s",
            "OU": "system"
        }
    ]
}
EOF


# 生成证书
cfssl gencert -ca=~/work/k8s/ca.pem -ca-key=~/work/k8s/ca-key.pem -config=~/work/k8s/ca-config.json -profile=kubernetes ~/work/k8s/kube-apiserver-csr.json | cfssljson -bare kube-apiserver

ll kube-apiserver*.pem

# -rw-------. 1 root root 1675 1月   4 21:51 kube-apiserver-key.pem
# -rw-r--r--. 1 root root 1659 1月   4 21:51 kube-apiserver.pem

# 生成token
sudo bash -c "cat > /usr/local/k8s/cfg/token.csv" << EOF
$(head -c 16 /dev/urandom | od -An -t x | tr -d ' '),kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF


sudo cp ~/work/k8s/ca*.pem  ~/work/k8s/kube-apiserver*.pem /usr/local/k8s/ssl/

```

***注意***

* --kubelet-certificate-authority=/usr/local/k8s/ssl/ca.pem 这个参数不需要设置如果使用token自动签发证书

```shell
sudo bash -c "cat > /usr/local/k8s/cfg/kube-apiserver.conf" << EOF
KUBE_APISERVER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/usr/local/k8s/logs \\
--etcd-servers=https://192.168.11.128:2379,https://192.168.11.129:2379,https://192.168.11.130:2379 \\
--bind-address=0.0.0.0 \\
--secure-port=6443 \\
--advertise-address=192.168.11.128 \\
--allow-privileged=true \\
--service-cluster-ip-range=10.0.0.0/16 \\
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\
--authorization-mode=RBAC,Node \\
--enable-bootstrap-token-auth=true \\
--token-auth-file=/usr/local/k8s/cfg/token.csv \\
--service-node-port-range=30000-32767 \\
--kubelet-client-certificate=/usr/local/k8s/ssl/kube-apiserver.pem \\
--kubelet-client-key=/usr/local/k8s/ssl/kube-apiserver-key.pem \\
--tls-cert-file=/usr/local/k8s/ssl/kube-apiserver.pem  \\
--tls-private-key-file=/usr/local/k8s/ssl/kube-apiserver-key.pem \\
--client-ca-file=/usr/local/k8s/ssl/ca.pem \\
--service-account-key-file=/usr/local/k8s/ssl/ca-key.pem \\
--service-account-signing-key-file=/usr/local/k8s/ssl/ca-key.pem \\
--service-account-issuer=https://kubernetes.default.svc.cluster.local \\
--etcd-cafile=/opt/etcd/ssl/ca.pem \\
--etcd-certfile=/usr/local/etcd/ssl/etcd.pem \\
--etcd-keyfile=/usr/local/etcd/ssl/etcd-key.pem \\
--audit-log-maxage=30 \\
--audit-log-maxbackup=3 \\
--audit-log-maxsize=100 \\
--audit-log-path=/usr/local/k8s/logs/k8s-audit.log"
EOF

# 注:上面两个\ \ 第一个是转义符，第二个是换行符，使用转义符是为了使用EOF保留换行符。
#–logtostderr：启用日志
#—v：日志等级
#–log-dir：日志目录
#–etcd-servers：etcd集群地址
#–bind-address：监听地址
#–secure-port：https安全端口
#–advertise-address：集群通告地址
#–allow-privileged：启用授权
#–service-cluster-ip-range：Service虚拟IP地址段
#–enable-admission-plugins：准入控制模块
#–authorization-mode：认证授权，启用RBAC授权和节点自管理
#–enable-bootstrap-token-auth：启用TLS bootstrap机制
#–token-auth-file：bootstrap token文件
#–service-node-port-range：Service nodeport类型默认分配端口范围
#–kubelet-client-xxx：apiserver访问kubelet客户端证书
#–tls-xxx-file：apiserver https证书
#–etcd-xxxfile：连接Etcd集群证书
#–audit-log-xxx：审计日志

```

#### systemd管理kube-apiserver

```shell

sudo bash -c "cat > /usr/lib/systemd/system/kube-apiserver.service" << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/usr/local/k8s/cfg/kube-apiserver.conf
ExecStart=/usr/local/k8s/bin/kube-apiserver $KUBE_APISERVER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload && sudo systemctl enable kube-apiserver && sudo systemctl start kube-apiserver
sudo systemctl status kube-apiserver -l

# 有这个错误不影响操作
#
#Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registr>

# 有返回说明启动正常
curl --insecure https://192.168.11.128:6443/

```

### 部署kubectl

***注意***

* 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权；
* kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role
  授予了调用kube-apiserver 的所有 API的权限；
* O指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:
  masters，所以被授予访问所有 API 的权限；
* 这个admin 证书，是将来生成管理员用的kube config 配置文件用的，现在我们一般建议使用RBAC 来对kubernetes 进行角色权限控制， kubernetes 将证书中的CN 字段 作为User， O 字段作为
  Group；
* “O”: “system:masters”, 必须是system:masters，否则后面kubectl create clusterrolebinding报错。

```shell
sudo bash -c "cat > ~/work/k8s/admin-csr.json" << EOF
{
  "CN": "admin",
  "hosts": [],
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "system:masters",             
      "OU": "system"
    }
  ]
}
EOF

# 生成证书

cfssl gencert -ca=~/work/k8s/ca.pem -ca-key=~/work/k8s/ca-key.pem -config=~/work/k8s/ca-config.json -profile=kubernetes ~/work/k8s/admin-csr.json | cfssljson -bare admin

sudo cp ~/work/k8s/admin*.pem /usr/local/k8s/ssl/

```

#### 创建kubeconfig配置文件

> kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书

```shell

cd /usr/local/k8s/cfg/

# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/usr/local/k8s/ssl/ca.pem --embed-certs=true --server=https://192.168.11.128:6443 --kubeconfig=kube.config

# 设置客户端认证参数
kubectl config set-credentials admin --client-certificate=/usr/local/k8s/ssl/admin.pem --client-key=/usr/local/k8s/ssl/admin-key.pem --embed-certs=true --kubeconfig=kube.config

# 设置上下文参数
kubectl config set-context kubernetes --cluster=kubernetes --user=admin --kubeconfig=kube.config

# 设置默认上下文
kubectl config use-context kubernetes --kubeconfig=kube.config

sudo mkdir ~/.kube

sudo cp /usr/local/k8s/cfg/kube.config  ~/.kube/config

# 授权kubernetes证书访问kubelet api权限

kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes


```

#### 查看集群组件状态

上面步骤完成后，kubectl就可以与kube-apiserver通信了

```shell

kubectl cluster-info

kubectl get componentstatuses

kubectl get all --all-namespaces

```

#### 查看集群组件状态，同步kubectl配置文件到其他节点

```shell

sudo scp -r ~/.kube/config  root@192.168.154.129:~/.kube/config
sudo scp -r ~/.kube/config  root@192.168.154.130:~/.kube/config

```

#### 配置kubectl子命令补全

```shell

sudo yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
kubectl completion bash > ~/.kube/completion.bash.inc
source '~/.kube/completion.bash.inc'  
source $HOME/.bash_profile


```

### 部署kube-controller-manager

#### 创建csr请求文件

```shell
sudo bash -c "cat > ~/work/k8s/kube-controller-manager-csr.json" << EOF
{
    "CN": "system:kube-controller-manager",
        "hosts": [
        "10.0.0.1",
        "127.0.0.1",
        "192.168.1.170",
        "192.168.1.171",
        "192.168.1.172",
        "192.168.1.173",
        "192.168.1.174",
        "192.168.1.175",
        "192.168.1.176",
        "192.168.56.133",
        "192.168.56.134",
        "192.168.56.135",
        "192.168.56.136",
        "192.168.56.137",
        "192.168.56.138",
        "192.168.56.139",
        "192.168.56.140",
        "192.168.56.141",
        "192.168.56.142",
        "192.168.56.143",
        "192.168.56.145",
        "192.168.154.125",
        "192.168.154.126",
        "192.168.154.127",
        "192.168.154.128",
        "192.168.154.129",
        "192.168.154.130",
        "192.168.154.131",
        "192.168.154.132",
        "192.168.154.133",
        "192.168.154.134",
        "192.168.154.135",
        "192.168.154.136",
        "192.168.154.137",
        "192.168.154.138",
        "192.168.154.139",
        "kubernetes",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "system:kube-controller-manager",
            "OU": "system"
        }
    ]
}
EOF

```

***注意***

* hosts 列表包含所有 kube-controller-manager 节点 IP；
* CN 为 system:kube-controller-manager、O 为 system:kube-controller-manager，kubernetes 内置的 ClusterRoleBindings system:
  kube-controller-manager 赋予 kube-controller-manager 工作所需的权限

生成证书

```shell

cfssl gencert -ca=ca.pem -ca-key=~/work/k8s/ca-key.pem -config=~/work/k8s/ca-config.json -profile=kubernetes ~/work/k8s/kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

ls kube-controller-manager*.pem

cp ~/work/k8s/kube-controller-manager*.pem /usr/local/k8s/ssl/

```

#### 创建kube-controller-manager的kubeconfig

```shell

cd /usr/local/k8s/cfg/

# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/usr/local/k8s/ssl/ca.pem --embed-certs=true --server=https://192.168.11.128:6443 --kubeconfig=kube-controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-controller-manager --client-certificate=/usr/local/k8s/ssl/kube-controller-manager.pem --client-key=/usr/local/k8s/ssl/kube-controller-manager-key.pem --embed-certs=true --kubeconfig=kube-controller-manager.kubeconfig

# 设置上下文参数
kubectl config set-context system:kube-controller-manager --cluster=kubernetes --user=system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig


```

#### 创建kube-controller-manager的配置文件

* --cluster-cidr=10.24.0.0/16 这个是pod的ip段
* --service-cluster-ip-range=10.0.0.0/16 service 的ip段

```shell
sudo bash -c  "cat > /usr/local/k8s/cfg/kube-controller-manager.conf" << EOF
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \\
--v=2 \\
--kubeconfig=/usr/local/k8s/cfg/kube-controller-manager.kubeconfig \\
--log-dir=/usr/local/k8s/logs \\
--leader-elect=true \\
--master=https://192.168.154.128:6443 \\
--bind-address=127.0.0.1 \\
--allocate-node-cidrs=true \\
--cluster-cidr=10.24.0.0/16 \\
--cluster-name=kubernetes \\
--client-ca-file=/usr/local/k8s/ssl/ca.pem \\
--service-cluster-ip-range=10.0.0.0/16 \\
--use-service-account-credentials=true \\
--horizontal-pod-autoscaler-use-rest-clients=true \\
--horizontal-pod-autoscaler-sync-period=10s \\
--feature-gates=RotateKubeletServerCertificate=true \\
--experimental-cluster-signing-duration=87600h \\
--controllers=*,bootstrapsigner,tokencleaner \\
--cluster-signing-cert-file=/usr/local/k8s/ssl/ca.pem \\
--cluster-signing-key-file=/usr/local/k8s/ssl/ca-key.pem \\
--tls-cert-file=/usr/local/k8s/ssl/kube-controller-manager.pem \\
--tls-private-key-file=/usr/local/k8s/ssl/kube-controller-manager-key.pem \\
--root-ca-file=/usr/local/k8s/ssl/ca.pem \\
--service-account-private-key-file=/usr/local/k8s/ssl/ca-key.pem"
EOF


#–master：通过本地非安全本地端口8080连接apiserver。
#–leader-elect：当该组件启动多个时，自动选举（HA）

```

#### systemd管理controller-manager 创建启动文件

```shell

sudo bash -c "cat > /usr/lib/systemd/system/kube-controller-manager.service" << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/usr/local/k8s/cfg/kube-controller-manager.conf
ExecStart=/usr/local/k8s/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF


sudo systemctl daemon-reload && sudo systemctl enable kube-controller-manager && sudo systemctl start kube-controller-manager
sudo systemctl status kube-controller-manager

```

### 部署kube-scheduler

#### 创建kube-scheduler csr请求文件

* "10.0.0.1", service 的ip端起始值

```shell

sudo bash -c "cat > ~/work/k8s/kube-scheduler-csr.json" << EOF
{
    "CN": "system:kube-scheduler",
        "hosts": [
        "10.0.0.1",
        "127.0.0.1",
        "192.168.1.170",
        "192.168.1.171",
        "192.168.1.172",
        "192.168.1.173",
        "192.168.1.174",
        "192.168.1.175",
        "192.168.1.176",
        "192.168.56.133",
        "192.168.56.134",
        "192.168.56.135",
        "192.168.56.136",
        "192.168.56.137",
        "192.168.56.138",
        "192.168.56.139",
        "192.168.56.140",
        "192.168.56.141",
        "192.168.56.142",
        "192.168.56.143",
        "192.168.56.145",
        "192.168.154.125",
        "192.168.154.126",
        "192.168.154.127",
        "192.168.154.128",
        "192.168.154.129",
        "192.168.154.130",
        "192.168.154.131",
        "192.168.154.132",
        "192.168.154.133",
        "192.168.154.134",
        "192.168.154.135",
        "192.168.154.136",
        "192.168.154.137",
        "192.168.154.138",
        "192.168.154.139",
        "kubernetes",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "system:kube-scheduler",
            "OU": "system"
        }
    ]
}
EOF

```

***注意***

* hosts 列表包含所有 kube-scheduler 节点 IP；
* CN 为 system:kube-scheduler、O 为 system:kube-scheduler，kubernetes 内置的 ClusterRoleBindings system:kube-scheduler 将赋予
  kube-scheduler 工作所需的权限

#### kube-scheduler创建生成证书

```shell

cfssl gencert -ca=ca.pem -ca-key=~/work/k8s/ca-key.pem -config=~/work/k8s/ca-config.json -profile=kubernetes ~/work/k8s/kube-scheduler-csr.json | cfssljson -bare kube-scheduler

ls kube-scheduler*.pem

sudo cp ~/work/k8s/kube-scheduler*.pem /usr/local/k8s/ssl/

```

#### 创建kube-scheduler的kubeconfig

```shell

cd /usr/local/k8s/cfg/

# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/usr/local/k8s/ssl/ca.pem --embed-certs=true --server=https://192.168.11.128:6443 --kubeconfig=kube-scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials system:kube-scheduler --client-certificate=/usr/local/k8s/ssl/kube-scheduler.pem --client-key=/usr/local/k8s/ssl/kube-scheduler-key.pem --embed-certs=true --kubeconfig=kube-scheduler.kubeconfig

# 设置上下文参数
kubectl config set-context system:kube-scheduler --cluster=kubernetes --user=system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig

# 设置默认上下文
kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig


```

#### 创建kube-controller-manager的配置文件

```shell

sudo bash -c  "cat > /usr/local/k8s/cfg/kube-scheduler.conf" << EOF
KUBE_SCHEDULER_OPTS="--logtostderr=false \
--v=2 \
--kubeconfig=/usr/local/k8s/cfg/kube-scheduler.kubeconfig \
--log-dir=/usr/local/k8s/logs \
--leader-elect=true \
--address=127.0.0.1 \
--master=https://192.168.154.128:6443 \
--bind-address=0.0.0.0"
EOF

```

#### kube-controller-manager 创建服务启动文件

```shell

sudo bash -c  "cat > /usr/lib/systemd/system/kube-scheduler.service" << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
EnvironmentFile=/usr/local/k8s/cfg/kube-scheduler.conf
ExecStart=/usr/local/k8s/bin/kube-scheduler $KUBE_SCHEDULER_OPTS
Restart=on-failure

[Install]
WantedBy=multi-user.target
EOF

sudo systemctl daemon-reload && sudo systemctl enable kube-scheduler && sudo systemctl start kube-scheduler
sudo systemctl status kube-scheduler

```

### 部署kubelet

#### 创建kubelet-bootstrap.kubeconfig master 上面执行

```shell

cd /usr/local/k8s/cfg/

# 设置变量 BOOTSTRAP_TOKEN
BOOTSTRAP_TOKEN=$(awk -F "," '{print $1}' /usr/local/k8s/cfg/token.csv)

# 设置集群参数
kubectl config set-cluster kubernetes --certificate-authority=/usr/local/k8s/ssl/ca.pem --embed-certs=true --server=https://192.168.11.128:6443 --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap --token=${BOOTSTRAP_TOKEN} --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default --cluster=kubernetes --user=kubelet-bootstrap --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig

# 创建角色绑定
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap

sudo scp /usr/local/k8s/cfg/kubelet-bootstrap.kubeconfig  root@192.168.154.129:/usr/local/k8s/cfg/kubelet-bootstrap.kubeconfig
sudo scp /usr/local/k8s/cfg/kubelet-bootstrap.kubeconfig  root@192.168.154.130:/usr/local/k8s/cfg/kubelet-bootstrap.kubeconfig


```

#### kubelet 创建配置文件

***注意****

* 这个文件将会转移到node节点
* address 应该是对应node节点ip
* "10.0.0.2" dns 与service同一个网段

```shell

sudo bash -c  "cat > /usr/local/k8s/cfg/kubelet.json" << EOF
{
    "kind": "KubeletConfiguration",
    "apiVersion": "kubelet.config.k8s.io/v1beta1",
    "authentication": {
        "x509": {
            "clientCAFile": "/usr/local/k8s/ssl/ca.pem"
        },
        "webhook": {
            "enabled": true,
            "cacheTTL": "2m0s"
        },
        "anonymous": {
          "enabled": false
        }
    },
    "authorization": {
        "mode": "Webhook",
        "webhook": {
            "cacheAuthorizedTTL": "5m0s",
            "cacheUnauthorizedTTL": "30s"
        }
    },
    "address": "192.168.154.129", # 这个值是node节点ip
    "port": 10250,
    "readOnlyPort": 10255,
    "cgroupDriver": "cgroupfs",  # 如果docker的驱动为systemd，处修改为systemd。此处设置很重要，否则后面node节点无法加入到集群
    "hairpinMode": "promiscuous-bridge",
    "serializeImagePulls": false,
    "featureGates": {
        "RotateKubeletClientCertificate": true,
        "RotateKubeletServerCertificate": true
    },
    "clusterDomain": "cluster.local.",
    "clusterDNS": [
        "10.0.0.2"
    ]
}
EOF

sudo scp /usr/local/k8s/cfg/kubelet.json  root@192.168.154.129:/usr/local/k8s/cfg/kubelet.json
sudo scp /usr/local/k8s/cfg/kubelet.json  root@192.168.154.130:/usr/local/k8s/cfg/kubelet.json

# 或

sudo bash -c  "cat > /usr/local/k8s/cfg/kubelet.yaml" << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /opt/kubernetes/ssl/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF

sudo scp /usr/local/k8s/cfg/kubelet.yaml  root@192.168.154.129:/usr/local/k8s/cfg/kubelet.yaml
sudo scp /usr/local/k8s/cfg/kubelet.yaml  root@192.168.154.130:/usr/local/k8s/cfg/kubelet.yaml

```

#### kubelet服务创建配置文件

***注意****

* 这个文件将会转移到node节点

```shell

sudo bash -c  "cat > /usr/local/k8s/cfg/kubelet.conf" << EOF
KUBELET_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/usr/local/k8s/logs \\
--network-plugin=cni \\
--alsologtostderr=true \\
--kubeconfig=/usr/local/k8s/cfg/kubelet.kubeconfig \\
--config=/usr/local/k8s/cfg/kubelet.json  \\
--bootstrap-kubeconfig=/usr/local/k8s/cfg/kubelet-bootstrap.kubeconfig \\
--cert-dir=/usr/local/k8s/ssl \\
--pod-infra-container-image=k8s.gcr.io/pause:3.2"
EOF

# –hostname-override：显示名称，集群中唯一
# –network-plugin：启用CNI
# –kubeconfig：空路径，会自动生成，后面用于连接apiserver
# –bootstrap-kubeconfig：首次启动向apiserver申请证书
# –config：配置参数文件
# –cert-dir：kubelet证书生成目录
# –pod-infra-container-image：管理Pod网络容器的镜像


sudo bash -c  "cat > /usr/local/k8s/cfg/kubelet.service" << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/usr/local/k8s/cfg/kubelet.conf
ExecStart=/usr/local/k8s/bin/kubelet $KUBELET_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

sudo scp /usr/local/k8s/cfg/kubelet.conf  root@192.168.154.129:/usr/local/k8s/cfg/kubelet.conf
sudo scp /usr/local/k8s/cfg/kubelet.conf  root@192.168.154.130:/usr/local/k8s/cfg/kubelet.conf

sudo scp /usr/local/k8s/cfg/kubelet.service  root@192.168.154.129:/usr/lib/systemd/system/kubelet.service
sudo scp /usr/local/k8s/cfg/kubelet.service  root@192.168.154.130:/usr/lib/systemd/system/kubelet.service

```

#### 启动服务 各个work节点上操作

```shell

sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet
sudo systemctl status kubelet

```

#### 确认kubelet服务启动成功后，接着到master上Approve一下bootstrap请求。执行如下命令可以看到三个worker节点分别发送了三个 CSR 请求：

```shell

kubectl get csr

# 审核通过
kubectl certificate approve node-csr-HlX3cExsZohWsu8Dd6Rp_ztFejmMdpzvti_qgxo4SAQ

kubectl get csr

kubectl get nodes

```

### 部署kube-proxy master 操作然后复制到node节点

```shell

sudo bash -c "cat > ~/work/k8s/kube-proxy-csr.json" << EOF
{
  "CN": "system:kube-proxy",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "L": "BeiJing",
      "ST": "BeiJing",
      "O": "k8s",
      "OU": "system"
    }
  ]
}
EOF

```

#### kube-proxy 生成证书

```shell

cd /urs/local/k8s/cfg/
cfssl gencert -ca=/urs/local/k8s/ssl/ca.pem -ca-key=/urs/local/k8s/ssl/ca-key.pem -config=/urs/local/k8s/ssl/ca-config.json -profile=kubernetes /urs/local/k8s/ssl/kube-proxy-csr.json | cfssljson -bare kube-proxy

ls kube-proxy*.pem

sudo cp ~/k8s/kube-proxy*.pem /urs/local/k8s/ssl/

```

#### kube-proxy 创建kubeconfig文件

```shell

kubectl config set-cluster kubernetes --certificate-authority=/urs/local/k8s/ssl/ca.pem --embed-certs=true --server=https://192.168.11.128:6443 --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy --client-certificate=/urs/local/k8s/ssl/kube-proxy.pem --client-key=/urs/local/k8s/ssl/kube-proxy-key.pem --embed-certs=true --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default --cluster=kubernetes --user=kube-proxy --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

sudo scp /usr/local/k8s/cfg/kube-proxy.kubeconfig  root@192.168.154.129:/usr/local/k8s/cfg/kube-proxy.kubeconfig
sudo scp /usr/local/k8s/cfg/kube-proxy.kubeconfig  root@192.168.154.130:/usr/local/k8s/cfg/kube-proxy.kubeconfig

```

#### 创建kube-proxy配置文件

```shell

sudo bash -c "cat > /urs/local/k8s/cfg/kube-proxy.yaml" << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  kubeconfig: /usr/local/k8s/cfg/kube-proxy.kubeconfig
clusterCIDR: 10.24.0.0/16 # 此处网段必须与网络组件网段保持一致，否则部署网络组件时会报 这个是pod的ip段
mode: ipvs
EOF

sudo scp /usr/local/k8s/cfg/kube-proxy.yaml  root@192.168.154.129:/usr/local/k8s/cfg/kube-proxy.yaml
sudo scp /usr/local/k8s/cfg/kube-proxy.yaml  root@192.168.154.130:/usr/local/k8s/cfg/kube-proxy.yaml

```

#### 创建kube-proxy服务配置文件

```shell

sudo bash -c  "cat > /usr/local/k8s/cfg/kube-proxy.conf" << EOF
KUBE_PROXY_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/usr/local/k8s/logs \\
--config=/usr/local/k8s/cfg/kube-proxy-config.yml"
EOF


sudo bash -c  "cat > /usr/local/k8s/cfg/kube-proxy.service" << EOF
[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
EnvironmentFile=/usr/local/k8s/cfg/kube-proxy.conf
ExecStart=/usr/local/k8s/bin/kube-proxy $KUBE_PROXY_OPTS
Restart=on-failure
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

sudo scp /usr/local/k8s/cfg/kube-proxy.yaml  root@192.168.154.129:/usr/local/k8s/cfg/kube-proxy.yaml
sudo scp /usr/local/k8s/cfg/kube-proxy.yaml  root@192.168.154.130:/usr/local/k8s/cfg/kube-proxy.yaml

sudo scp /usr/local/k8s/cfg/kube-proxy.service  root@192.168.154.129:/usr/lib/systemd/system/kube-proxy.service
sudo scp /usr/local/k8s/cfg/kube-proxy.service  root@192.168.154.130:/usr/lib/systemd/system/kube-proxy.service

```

#### kube-proxy启动服务 各个work节点上操作

```shell

sudo systemctl daemon-reload && sudo systemctl enable kube-proxy && sudo systemctl start kube-proxy
sudo systemctl status kube-proxy

```

### 配置网络组件 master

```shell

wget https://docs.projectcalico.org/v3.14/manifests/calico.yaml
kubectl apply -f calico.yaml 

# 此时再来查看各个节点，均为Ready状态

kubectl get pods -A
kubectl get nodes

```

***注意*** 网段与 --cluster-cid clusterCIDR 一致 这个是pod的ip段

- name: CALICO_IPV4POOL_CIDR value: "10.24.0.0/16"

### master 上面操作 部署coredns

下载coredns yaml文件： https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed

***注意*** 修改yaml文件:

* kubernetes cluster.local in-addr.arpa ip6.arpa
* forward . /etc/resolv.conf
* clusterIP为：10.0.0.2 （kubelet配置文件中的clusterDNS） 与service同一个网段

```yaml
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
  labels:
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: Reconcile
  name: system:coredns
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
      - services
      - pods
      - namespaces
    verbs:
      - list
      - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
    addonmanager.kubernetes.io/mode: EnsureExists
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
  - kind: ServiceAccount
    name: coredns
    namespace: kube-system
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
  labels:
    addonmanager.kubernetes.io/mode: EnsureExists
data:
  Corefile: |
    .:53 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  # replicas: not specified here:
  # 1. In order to make Addon Manager do not reconcile this replicas parameter.
  # 2. Default is 1.
  # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      serviceAccountName: coredns
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      containers:
        - name: coredns
          image: coredns/coredns:1.8.0
          imagePullPolicy: IfNotPresent
          resources:
            limits:
              memory: 170Mi
            requests:
              cpu: 100m
              memory: 70Mi
          args: [ "-conf", "/etc/coredns/Corefile" ]
          volumeMounts:
            - name: config-volume
              mountPath: /etc/coredns
              readOnly: true
          ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - all
            readOnlyRootFilesystem: true
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
              - key: Corefile
                path: Corefile
---
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.0.0.2 # 这个地方
  ports:
    - name: dns
      port: 53
      protocol: UDP
    - name: dns-tcp
      port: 53
      protocol: TCP

```

```shell

kubectl apply -f coredns.yaml 

```

#### ingress-nginx 安装

参考地址：

* [Ingress Nginx项目地址](https://github.com/kubernetes/ingress-nginx/)
* [Ingress Nginx的安装文档](https://kubernetes.github.io/ingress-nginx/deploy/)
* [使用说明](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/)

> deploy.yaml 这个文件因为镜像拉去不了而且是1.1本版的ingress-nginx，我的k8s是1.20.1使用需要使用1.0，ingress-nginx.yaml是1.0版本
> 镜像需要使用hub.docker里面的，因为k8s.gcr.io不能访问

##### 启动 ingress-nginx

```shell

kubectl create -f  ingress-nginx.yaml

```

##### 查看是否已完成

~~~

[zr@master01 file]$ kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-njhsj        0/1     Completed   0          15h
ingress-nginx-admission-patch-sxm5p         0/1     Completed   1          15h
ingress-nginx-controller-7d4df87d89-tcnln   1/1     Running     0          15h

~~~

##### 测试ingress-nginx

##### 生成pod与service

~~~

[zr@master01 file]$ kubectl create -f nginx.yaml
replicationcontroller/nginx-controller created
service/nginx-service-nodeport created

~~~

##### 查看 pod ip是pod的ip

~~~

[zr@master01 file]$ kubectl get pods -o wide
NAME                     READY   STATUS    RESTARTS   AGE    IP              NODE     NOMINATED NODE   READINESS GATES
nginx-controller-4hbxh   1/1     Running   0          5m9s   10.24.140.80    node02   <none>           <none>
nginx-controller-pfkjw   1/1     Running   0          5m9s   10.24.140.81    node02   <none>           <none>
web-96d5df5c8-d297j      1/1     Running   3          18h    10.24.196.136   node01   <none>           <none>

~~~

##### 查看 service CLUSTER-IP 是service ip

~~~

[zr@master01 file]$ kubectl get svc -o wide
NAME                     TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE     SELECTOR
kubernetes               ClusterIP   10.0.0.1      <none>        443/TCP          8d      <none>
nginx-service-nodeport   NodePort    10.0.141.48   <none>        8080:30005/TCP   4m28s   name=nginx
web                      NodePort    10.0.76.253   <none>        80:34529/TCP     41h     app=web

~~~

##### 创建对应的 ingress

~~~

[zr@master01 file]$ kubectl create -f http-nginx-ingress.yaml
ingress.networking.k8s.io/http-nginx-ingress created


[zr@master01 file]$ kubectl get svc -A
NAMESPACE       NAME                                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
default         kubernetes                           ClusterIP   10.0.0.1       <none>        443/TCP                      8d
default         nginx-service-nodeport               NodePort    10.0.136.158   <none>        8080:30005/TCP               6m9s
default         web                                  NodePort    10.0.76.253    <none>        80:34529/TCP                 41h
ingress-nginx   ingress-nginx-controller             NodePort    10.0.64.224    <none>        80:31093/TCP,443:35332/TCP   16h
ingress-nginx   ingress-nginx-controller-admission   ClusterIP   10.0.245.61    <none>        443/TCP                      16h
kube-system     kube-dns                             ClusterIP   10.0.0.2       <none>        53/UDP,53/TCP                5d22h

[zr@master01 file]$ kubectl get pods -A -o wide
NAMESPACE       NAME                                        READY   STATUS      RESTARTS   AGE     IP                NODE     NOMINATED NODE   READINESS GATES
default         nginx-controller-9blk9                      1/1     Running     0          6m13s   10.24.140.82      node02   <none>           <none>
default         nginx-controller-cng7j                      1/1     Running     0          6m13s   10.24.196.140     node01   <none>           <none>
default         nginx-controller-drmnp                      1/1     Running     0          6m13s   10.24.196.139     node01   <none>           <none>
default         web-96d5df5c8-d297j                         1/1     Running     3          19h     10.24.196.136     node01   <none>           <none>
ingress-nginx   ingress-nginx-admission-create-njhsj        0/1     Completed   0          16h     10.24.140.77      node02   <none>           <none>
ingress-nginx   ingress-nginx-admission-patch-sxm5p         0/1     Completed   1          16h     10.24.140.78      node02   <none>           <none>
ingress-nginx   ingress-nginx-controller-7d4df87d89-tcnln   1/1     Running     0          16h     10.24.140.79      node02   <none>           <none>
kube-system     calico-kube-controllers-574f577444-jgrfj    1/1     Running     6          19h     10.24.196.135     node01   <none>           <none>
kube-system     calico-node-f5jp2                           1/1     Running     3          19h     192.168.154.129   node01   <none>           <none>
kube-system     calico-node-hjgkl                           1/1     Running     3          19h     192.168.154.130   node02   <none>           <none>
kube-system     coredns-5fd5d5b6d5-86sbg                    1/1     Running     7          5d22h   10.24.140.67      node02   <none>           <none>

[zr@master01 file]$ kubectl get ingress -A
NAMESPACE   NAME                 CLASS    HOSTS             ADDRESS           PORTS   AGE
default     http-nginx-ingress   <none>   mynignx.cn        192.168.154.130   80      10m
default     tomcat               <none>   tomcat.cnsre.cn   192.168.154.130   80      15h


~~~

